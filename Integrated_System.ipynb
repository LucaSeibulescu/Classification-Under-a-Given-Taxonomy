{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "Collecting torch\n",
      "  Using cached torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.42.1)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (1.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.0.1)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (2.2.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Installing collected packages: regex, packaging, tokenizers, sacremoses, huggingface-hub, transformers, torch\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.1\n",
      "    Uninstalling packaging-20.1:\n",
      "      Successfully uninstalled packaging-20.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\n",
      "Successfully installed huggingface-hub-0.5.1 packaging-21.3 regex-2022.3.15 sacremoses-0.0.49 tokenizers-0.12.1 torch-1.11.0 transformers-4.18.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Error parsing requirements for wcwidth: [Errno 2] No such file or directory: '/opt/conda/lib/python3.7/site-packages/wcwidth-0.1.8.dist-info/METADATA'\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.1.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from nltk) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "These are the necessary imports and downloads needed for the project to run\n",
    "'''\n",
    "\n",
    "\n",
    "!pip install transformers torch\n",
    "!pip install git+https://github.com/PrithivirajDamodaran/ZSIC.git -q\n",
    "!pip install gensim\n",
    "!pip install nltk\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "import operator\n",
    "import itertools\n",
    "import string\n",
    "import csv\n",
    "import transformers\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn import metrics\n",
    "from PIL import Image\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Outlier Filtration </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the dictionary that is going to store all the outliers with acceptable pictures\n",
    "'''\n",
    "\n",
    "all_outliers_sku = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Filter Outliers from original dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is the first step in finding all the outliers. An outlier is a SKU where the SKU has a segment name that is \n",
    "in the bottom 5% relative to the other segment names, has a category name that is in the bottom 5% relative to \n",
    "the other category names, has a class name that is in the bottom 5% relative to the other class names, \n",
    "and has a subclass name that is in the bottom 5% relative to the other subclass names.\n",
    "\n",
    "seg_outliers is a list of the segment names that occurr 5% and below\n",
    "category_outliers is a list of the category names that occurr 5% and below\n",
    "cls_outliers is a list of the class names that occurr 5% and below\n",
    "subcls_outliers is a list of the subclass names that occurr 5% and below\n",
    "\n",
    "'''\n",
    "\n",
    "df=pd.read_csv(\"/root/Capstone_Dataset_final.csv\")\n",
    "dataset = open('/root/image_info_final.txt', 'r')\n",
    "relative_segments_counts = dict(df.groupby([\"Segment Name\"])[\"Segment Name\"].count())\n",
    "relative_category_counts = dict(df.groupby([\"Category Name\"])[\"Category Name\"].count())\n",
    "relative_class_counts = dict(df.groupby([\"Class Name\"])[\"Class Name\"].count())\n",
    "relative_subclass_counts = dict(df.groupby([\"Sub-Class Name\"])[\"Sub-Class Name\"].count())\n",
    "def find_outliers(vals,threshold):\n",
    "    X = dict(sorted(vals.items(),key=lambda label:label[1]))\n",
    "    outlier_amt = int(threshhold*len(X))\n",
    "    outliers = list(X.keys())[0:outlier_amt]\n",
    "    return outliers\n",
    "threshhold = .05\n",
    "seg_outliers = find_outliers(relative_segments_counts,threshhold)\n",
    "category_outliers = find_outliers(relative_category_counts,threshhold)\n",
    "cls_outliers = find_outliers(relative_class_counts,threshhold)\n",
    "subcls_outliers = find_outliers(relative_subclass_counts,threshhold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here we are gathering a list of all the SKU's that fall under a segment, category, class, \n",
    "or a subclass that is classified as an outlier\n",
    "\n",
    "\n",
    "outlier_sku is a list of all the SKU's that are outliers\n",
    "'''\n",
    "\n",
    "df=pd.read_csv(\"/root/Capstone_Dataset_final.csv\")\n",
    "dataset = open('/root/image_info_final.txt', 'r')\n",
    "outlier_sku = []\n",
    "for i in seg_outliers:   \n",
    "    for j in list(df.loc[df[\"Segment Name\"]==i][\"SKU\"]):\n",
    "        outlier_sku.append(j)\n",
    "for i in category_outliers:   \n",
    "    for j in list(df.loc[df[\"Category Name\"]==i][\"SKU\"]):\n",
    "        outlier_sku.append(j)\n",
    "for i in cls_outliers:   \n",
    "    for j in list(df.loc[df[\"Class Name\"]==i][\"SKU\"]):\n",
    "        outlier_sku.append(j)\n",
    "for i in subcls_outliers:   \n",
    "    for j in list(df.loc[df[\"Sub-Class Name\"]==i][\"SKU\"]):\n",
    "        outlier_sku.append(j)\n",
    "outlier_sku = list(set(outlier_sku))\n",
    "len(outlier_sku)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Find outliers with images</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we are mapping all of the SKU's to the SKU's accociated pictures, if it has one. In addition we go \n",
    "through all of the pictures and do a check if they are in an acceptable format.\n",
    "\n",
    "all_outliers_sku is a dictionary of the outlier SKU's that have accpetable pictures\n",
    "\n",
    "'''\n",
    "\n",
    "df=pd.read_csv(\"/root/Capstone_Dataset_final.csv\")\n",
    "dataset = open('/root/image_info_final.txt', 'r')\n",
    "SKUToProduct_seg = df.groupby('SKU')['Segment Name'].apply(list)\n",
    "SKUToProduct_cat = df.groupby('SKU')['Category Name'].apply(list)\n",
    "SKUToProduct_cls = df.groupby('SKU')['Class Name'].apply(list)\n",
    "SKUToProduct_subcls = df.groupby('SKU')['Sub-Class Name'].apply(list)\n",
    "segToImage_specific = collections.defaultdict(list)\n",
    "catToImage_specific = collections.defaultdict(list)\n",
    "clsToImage_specific = collections.defaultdict(list)\n",
    "subclsToImage_specific = collections.defaultdict(list)\n",
    "productDict_cat = collections.defaultdict(dict)\n",
    "productDict_seg = collections.defaultdict(dict)\n",
    "productDict_cls = collections.defaultdict(dict)\n",
    "productDict_subcls = collections.defaultdict(dict)\n",
    "\n",
    "pictures = []\n",
    "lines = dataset.readlines()[1:]\n",
    "i = 0\n",
    "for line in lines:\n",
    "    x = line.split(\" | \")\n",
    "    pictures.append(x)\n",
    "\n",
    "for picture in pictures:\n",
    "    sku = int(picture[0])\n",
    "    images = picture[1].lstrip().split(\" \")\n",
    "    \n",
    "    productName_seg = SKUToProduct_seg[sku][0]\n",
    "    productName_cat = SKUToProduct_cat[sku][0]\n",
    "    productName_cls = SKUToProduct_cls[sku][0]\n",
    "    productName_subcls = SKUToProduct_subcls[sku][0]\n",
    "    for img in images:\n",
    "        segToImage_specific[sku].append(img)\n",
    "        catToImage_specific[sku].append(img)\n",
    "        clsToImage_specific[sku].append(img)\n",
    "        subclsToImage_specific[sku].append(img)\n",
    "        \n",
    "    productDict_seg[productName_seg][sku] = segToImage_specific[sku]\n",
    "    productDict_cat[productName_cat][sku] = catToImage_specific[sku]\n",
    "    productDict_cls[productName_cls][sku] = clsToImage_specific[sku]\n",
    "    productDict_subcls[productName_subcls][sku] = subclsToImage_specific[sku]\n",
    "for i in seg_outliers:   \n",
    "    for j in productDict_seg[i].items():\n",
    "        all_outliers_sku[j[0]] = j[1]\n",
    "for i in category_outliers:   \n",
    "    for j in productDict_cat[i].items():\n",
    "        if j[0] not in all_outliers_sku:\n",
    "            all_outliers_sku[j[0]] = j[1]\n",
    "for i in cls_outliers:   \n",
    "    for j in productDict_cls[i].items():\n",
    "        if j[0] not in all_outliers_sku:\n",
    "            all_outliers_sku[j[0]] = j[1]\n",
    "for i in subcls_outliers:   \n",
    "    for j in productDict_subcls[i].items():\n",
    "        if j[0] not in all_outliers_sku:\n",
    "            all_outliers_sku[j[0]] = j[1]\n",
    "\n",
    "non_acceptable_pictures = []\n",
    "\n",
    "acceptable_format = [\"jp2\",\"jpeg\",\"png\",\"jp2\\n\",\"jpeg\\n\",\"png\\n\"]\n",
    "for i in all_outliers_sku.items():\n",
    "    for j in i[1]:\n",
    "        test = j.split(\".\")\n",
    "        if test[1] not in acceptable_format:\n",
    "            non_acceptable_pictures.append(i[0])\n",
    "            break\n",
    "\n",
    "for i in non_acceptable_pictures:\n",
    "    all_outliers_sku.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we are querying the original dataframe to get all of the outliers SKU's without pictures or outlier \n",
    "SKU's that do not have pictures in accpetable formats.\n",
    "\n",
    "df_outliers_no_pic is a dataframe of outliers with no pictures or outliers that have an unacceptable picture format\n",
    "'''\n",
    "\n",
    "\n",
    "df_outliers = df[df[\"SKU\"].isin(outlier_sku)]\n",
    "df_outliers_no_pic = df_outliers[~(df_outliers[\"SKU\"].isin(all_outliers_sku.keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29756"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[~df[\"SKU\"].isin(outlier_sku)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Outliers without images </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Here we are creating a dictionary for the outliers without a picture or outliers with an unnaceptable picture.\n",
    "The first choice is the SKU's Item Description, the second choice is the SKU's Item Description 2, the third choice is the SKU's Item Title,\n",
    "and the last choice is the SKU's Manufacturer name. If all 4 of those fields are null for a SKU then we add it to a seperate list.\n",
    "\n",
    "product_Dict_text is a dictionary that mapes SKU's to either its Item Description, Item Description 2, ItemTitle, or its manufacturer.\n",
    "no_des is a list of SKU's that do not have any pictures, acceptable pictures, and no acceotable text desciprtion\n",
    "'''\n",
    "\n",
    "\n",
    "SKUtoProduct_seg = df_outliers_no_pic.groupby(\"SKU\")[\"Segment Name\"].apply(list)\n",
    "SKUtoProduct_cat = df_outliers_no_pic.groupby(\"SKU\")[\"Category Name\"].apply(list)\n",
    "SKUtoProduct_cls = df_outliers_no_pic.groupby(\"SKU\")[\"Class Name\"].apply(list)\n",
    "SKUtoProduct_subcls = df_outliers_no_pic.groupby(\"SKU\")[\"Sub-Class Name\"].apply(list)\n",
    "\n",
    "SKUToDescription1 = df_outliers_no_pic.groupby(\"SKU\")[\"ItemDescription\"].apply(list)\n",
    "SKUToDescription2 = df_outliers_no_pic.groupby(\"SKU\")[\"ItemDescription_2\"].apply(list)\n",
    "SKUToDescription3 = df_outliers_no_pic.groupby(\"SKU\")[\"ItemTitle\"].apply(list)\n",
    "SKUToDescription4 = df_outliers_no_pic.groupby(\"SKU\")[\"Manufacturer\"].apply(list)\n",
    "\n",
    "product_Dict_text = {}\n",
    "\n",
    "SKU = list(df_outliers_no_pic[\"SKU\"])\n",
    "# print(SKU)\n",
    "# for i in non_acceptable_pictures:\n",
    "#     SKU.append(i)\n",
    "# print(SKU)\n",
    "no_des = []\n",
    "for sku in SKU:\n",
    "    productName_seg = SKUtoProduct_seg[sku][0]\n",
    "    productName_cat = SKUtoProduct_cat[sku][0]\n",
    "    productName_cls = SKUtoProduct_cls[sku][0]\n",
    "    productName_subcls = SKUtoProduct_subcls[sku][0]\n",
    "    \n",
    "    des1,des2,des3,des4 = SKUToDescription1.get(sku,[0.0])[0],SKUToDescription2.get(sku,[0.0])[0],SKUToDescription3.get(sku,[0.0])[0],SKUToDescription4.get(sku,[0.0])[0]\n",
    "    if type(des1)==type(\"string\"):\n",
    "        product_Dict_text[sku] = des1\n",
    "    elif type(des2)==type(\"string\"):\n",
    "        product_Dict_text[sku] = des2\n",
    "    elif type(des3)==type(\"string\"):\n",
    "        product_Dict_text[sku] = des3\n",
    "    elif type(des4)==type(\"string\"):\n",
    "        product_Dict_text[sku] = des4\n",
    "    else:\n",
    "        no_des.append(sku)\n",
    "len(product_Dict_text.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Initialize Text Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we are initializing the text based zero shot model\n",
    "'''\n",
    "\n",
    "\n",
    "classifier = transformers.pipeline(\"zero-shot-classification\",model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_Dict_text)\n",
    "len(all_outliers_sku)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Text Zero Shot Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKU 1 out of 21\n",
      "SKU 2 out of 21\n",
      "SKU 3 out of 21\n",
      "SKU 4 out of 21\n",
      "SKU 5 out of 21\n",
      "SKU 6 out of 21\n",
      "SKU 7 out of 21\n",
      "SKU 8 out of 21\n",
      "SKU 9 out of 21\n",
      "SKU 10 out of 21\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here we are using the text based zero shot model to predict the SKU's segment names, \n",
    "category names, class names, and subclass names based on the text descrtiption that the SKU is mapped to.\n",
    "We feed in all the unique segment names as the possible choices for the SKU's segment classification.\n",
    "We feed in all the unique category names as the possible choices for the SKU's category classification.\n",
    "We feed in all the unique class names as the possible choices for the SKU's class classification.\n",
    "We feed in all the unique subclass names as the possible choices for the SKU's subclass classification.\n",
    "\n",
    "text_outliers is a mapping of an SKU to a list of classifications/classifications confidence scores.\n",
    "Here is the format of a key-value pair in text outliers:\n",
    "SKU -> [[subclass name, subclass confidence],[class name, class confidence],[category name, category confidence],[segment name, segment confidence]]\n",
    "'''\n",
    "text_outliers = collections.defaultdict(list)\n",
    "\n",
    "labels_seg = list(df[\"Segment Name\"].unique())\n",
    "labels_cat = list(df[\"Category Name\"].unique())\n",
    "labels_cls = list(df[\"Class Name\"].unique())\n",
    "labels_subcls = list(df[\"Sub-Class Name\"].unique())\n",
    "count = 0\n",
    "for sku,des in product_Dict_text.items():\n",
    "    count+=1\n",
    "    print(\"SKU {} out of {}\".format(count,len(product_Dict_text)))\n",
    "    preds_seg = classifier(des, labels_seg, multi_label=True)\n",
    "    preds_cat = classifier(des, labels_cat, multi_label=True)\n",
    "    preds_cls = classifier(des, labels_cls, multi_label=True)\n",
    "    preds_subcls = classifier(des, labels_subcls, multi_label=True)\n",
    "    text_outliers[sku].append([preds_subcls['labels'][0],preds_subcls['scores'][0]])\n",
    "    text_outliers[sku].append([preds_cls['labels'][0],preds_cls['scores'][0]])\n",
    "    text_outliers[sku].append([preds_cat['labels'][0],preds_cat['scores'][0]])\n",
    "    text_outliers[sku].append([preds_seg['labels'][0],preds_seg['scores'][0]])\n",
    "    \n",
    "    \n",
    "    \n",
    "text_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Zero Shot Image Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we are initializing the image based zero shot model.\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from ZSIC import ZeroShotImageClassification\n",
    "\n",
    "zsic = ZeroShotImageClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we are using the image based zero shot model to predict the SKU's segment names, \n",
    "category names, class names, and subclass names based on the image or images that a SKU is a mapped too.\n",
    "If an SKU has multiple images then the image that produces the higest confidence score for a particular classification is used.\n",
    "\n",
    "We feed in all the unique segment names as the possible choices for the SKU's segment classification.\n",
    "We feed in all the unique category names as the possible choices for the SKU's category classification.\n",
    "We feed in all the unique class names as the possible choices for the SKU's class classification.\n",
    "We feed in all the unique subclass names as the possible choices for the SKU's subclass classification.\n",
    "\n",
    "accurate_segments is a dictionary of the SKU's where a SKU is mapped to the segment classification and segment classification confidence score\n",
    "accurate_category is a dictionary of the SKU's where a SKU is mapped to the cateogory classification and cateogry classification confidence score\n",
    "accurate_class is a dictionary of the SKU's where a SKU is mapped to the class classification and class classification confidence score\n",
    "accurate_subclass is a dictionary of the SKU's where a SKU is mapped to the subclass classification and subclass classification confidence score\n",
    "'''\n",
    "\n",
    "\n",
    "inp_image = \"/root/Capstone_images_set_final/\"\n",
    "labels_seg = list(df[\"Segment Name\"].unique())\n",
    "labels_cat = list(df[\"Category Name\"].unique())\n",
    "labels_cls = list(df[\"Class Name\"].unique())\n",
    "labels_subcls = list(df[\"Sub-Class Name\"].unique())\n",
    "\n",
    "correct = 0\n",
    "accurate_segments = collections.defaultdict(list)\n",
    "accurate_category = collections.defaultdict(list)\n",
    "accurate_class = collections.defaultdict(list)\n",
    "accurate_subclass = collections.defaultdict(list)\n",
    "count = 0\n",
    "for sku,picture in all_outliers_sku.items():\n",
    "    count+=1\n",
    "    print(\"SKU {} out of {}\".format(count,len(all_outliers_sku)))\n",
    "    for j in picture:\n",
    "        j = j.replace(\"\\n\",\"\")\n",
    "        j = Image.open(inp_image + j)\n",
    "\n",
    "        preds_seg = zsic(image=j,\n",
    "                    candidate_labels=labels_seg) \n",
    "        preds_cat = zsic(image=j,\n",
    "                    candidate_labels=labels_cat)\n",
    "        preds_cls = zsic(image=j,\n",
    "                    candidate_labels=labels_cls)\n",
    "        preds_subcls = zsic(image=j,\n",
    "                    candidate_labels=labels_subcls)\n",
    "        if sku not in accurate_segments:\n",
    "            accurate_segments[sku].append(preds_seg['labels'][0])\n",
    "            accurate_segments[sku].append(preds_seg['scores'][0])\n",
    "        else:\n",
    "            if preds_seg['scores'][0]>accurate_segments[sku][1]:\n",
    "                accurate_segments[sku][0] = preds_seg['labels'][0]\n",
    "                accurate_segments[sku][1] = preds_seg['scores'][0]\n",
    "    \n",
    "        if sku not in accurate_category:\n",
    "            accurate_category[sku].append(preds_cat['labels'][0])\n",
    "            accurate_category[sku].append(preds_cat['scores'][0])\n",
    "        else:\n",
    "            if preds_cat['scores'][0]>accurate_category[sku][1]:\n",
    "                accurate_category[sku][0] = preds_cat['labels'][0]\n",
    "                accurate_category[sku][1] = preds_cat['scores'][0]\n",
    "                \n",
    "        if sku not in accurate_class:\n",
    "            accurate_class[sku].append(preds_cls['labels'][0])\n",
    "            accurate_class[sku].append(preds_cls['scores'][0])\n",
    "        else:\n",
    "            if preds_cls['scores'][0]>accurate_class[sku][1]:\n",
    "                accurate_class[sku][0] = preds_cls['labels'][0]\n",
    "                accurate_class[sku][1] = preds_cls['scores'][0]\n",
    "                \n",
    "        if sku not in accurate_subclass:\n",
    "            accurate_subclass[sku].append(preds_subcls['labels'][0])\n",
    "            accurate_subclass[sku].append(preds_subcls['scores'][0])\n",
    "        else:\n",
    "            if preds_subcls['scores'][0]>accurate_subclass[sku][1]:\n",
    "                accurate_subclass[sku][0] = preds_subcls['labels'][0]\n",
    "                accurate_subclass[sku][1] = preds_subcls['scores'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we are orgainizing all the SKU's and their classifications along with their classification confidence scores.\n",
    "This includes all the SKU's that were processed with the image based zero shot model, and text based zero shot model.\n",
    "If an outlier SKU did not have an acceptable image or an appropriate text input then we go with the classification on \n",
    "the dataset with 100% confidence.\n",
    "\n",
    "outlier_accuracy is a mapping of an SKU to a list of classifications/classifications confidence scores.\n",
    "Here is the format of a key-value pair in outlier_accuracy:\n",
    "SKU -> [[subclass name, subclass confidence],[class name, class confidence],[category name, category confidence],[segment name, segment confidence]]\n",
    "\n",
    "'''\n",
    "outlier_accuracy = collections.defaultdict(list)\n",
    "for i in accurate_segments.items():\n",
    "    outlier_accuracy[i[0]].append(accurate_subclass[i[0]])\n",
    "    outlier_accuracy[i[0]].append(accurate_class[i[0]])\n",
    "    outlier_accuracy[i[0]].append(accurate_category[i[0]])\n",
    "    outlier_accuracy[i[0]].append(i[1])\n",
    "    \n",
    "\n",
    "for i in no_des:\n",
    "    outlier_accuracy[i].append([list(df_outliers_no_pic.loc[test[\"SKU\"]==i][\"Sub-Class Name\"]),1.0]) \n",
    "    outlier_accuracy[i].append([list(df_outliers_no_pic.loc[test[\"SKU\"]==i][\"Class Name\"]),1.0]) \n",
    "    outlier_accuracy[i].append([list(df_outliers_no_pic.loc[test[\"SKU\"]==i][\"Category Name\"]),1.0]) \n",
    "    outlier_accuracy[i].append([list(df_outliers_no_pic.loc[test[\"SKU\"]==i][\"Segment Name\"]),1.0]) \n",
    "    \n",
    "    \n",
    "for i in text_outliers.items():\n",
    "    outlier_accuracy[i[0]]=i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Unsupervised Learning Model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Read Data </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>ItemTitle</th>\n",
       "      <th>ItemDescription</th>\n",
       "      <th>ItemBulletPoint</th>\n",
       "      <th>ItemDescription_2</th>\n",
       "      <th>Manufacturer</th>\n",
       "      <th>MfrPartNum</th>\n",
       "      <th>SellUOM</th>\n",
       "      <th>ItemPrice</th>\n",
       "      <th>ItemFactTag</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Segment Name</th>\n",
       "      <th>Category</th>\n",
       "      <th>Category Name</th>\n",
       "      <th>Class</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Sub-Class</th>\n",
       "      <th>Sub-Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>131645</td>\n",
       "      <td>Gibson Studio California Mauna 12-Piece Melami...</td>\n",
       "      <td>Whether you're dining indoors or outdoors, thi...</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Melamine construction for rugged durab...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GIBSON OVERSEAS INC.</td>\n",
       "      <td>995105704M</td>\n",
       "      <td>EA</td>\n",
       "      <td>31.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEG_11000000</td>\n",
       "      <td>Food &amp; Beverage</td>\n",
       "      <td>CAT_11500000</td>\n",
       "      <td>Food &amp; Beverage Serving</td>\n",
       "      <td>CLS_11540000</td>\n",
       "      <td>Tableware</td>\n",
       "      <td>SUB_11540008</td>\n",
       "      <td>Sets - Dinnerware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42483</td>\n",
       "      <td>Thronmax X1 Webcam - 30 fps - USB 2.0 - 1 Pack...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Video conferencing couldn't get any be...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;Suitable for Most Software &lt;/b&gt;&lt;/p&gt; &lt;p&gt;P...</td>\n",
       "      <td>THRONMAX</td>\n",
       "      <td>X1</td>\n",
       "      <td>EA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEG_400000</td>\n",
       "      <td>Computers &amp; Hardware</td>\n",
       "      <td>CAT_420000</td>\n",
       "      <td>Computer Accessories</td>\n",
       "      <td>CLS_424000</td>\n",
       "      <td>Calibrators, Webcams &amp; Hubs</td>\n",
       "      <td>SUB_424003</td>\n",
       "      <td>Webcams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175678</td>\n",
       "      <td>IOGEAR Thunderbolt 3 USB-C 2m 20Gbps Cable - 6...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Power delivery up to 100W (20V @5A) us...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;Thunderbolt™ 3 - Premium Performance for...</td>\n",
       "      <td>ATEN TECHNOLOGIES</td>\n",
       "      <td>GT3C02</td>\n",
       "      <td>EA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEG_20000000</td>\n",
       "      <td>Cords &amp; Cables</td>\n",
       "      <td>CAT_20500000</td>\n",
       "      <td>Power &amp; Data</td>\n",
       "      <td>CLS_20510000</td>\n",
       "      <td>Cables - Data Transfer</td>\n",
       "      <td>SUB_20510002</td>\n",
       "      <td>Cables - Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190527</td>\n",
       "      <td>Hoffman Tech 677-12E-HTI Remanufactured Extra-...</td>\n",
       "      <td>Remanufactured from an original HP recycled ca...</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Comparable to the HP 12A (Q2612D) cart...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMAGE PROJECTIONS WEST, INC.</td>\n",
       "      <td>677-12E-HTI</td>\n",
       "      <td>EA</td>\n",
       "      <td>62.94</td>\n",
       "      <td>* Quality toner at a savings * Yields up to 4,...</td>\n",
       "      <td>SEG_24000000</td>\n",
       "      <td>Ink, Toner, Ribbons &amp; 3D Filaments</td>\n",
       "      <td>CAT_24400000</td>\n",
       "      <td>Toner</td>\n",
       "      <td>CLS_24410000</td>\n",
       "      <td>Machine Toner</td>\n",
       "      <td>SUB_5070132</td>\n",
       "      <td>Cartridges - Toner, Color</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145583</td>\n",
       "      <td>Suncast Commercial Paper Recycling Lid For 23-...</td>\n",
       "      <td>Keep recyclables (where facilities exist) cont...</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Made of durable resin.&lt;/li&gt; &lt;li&gt;Fits m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SUNCAST CORPORATION</td>\n",
       "      <td>TCNLID03BLD</td>\n",
       "      <td>EA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEG_14000000</td>\n",
       "      <td>Janitorial</td>\n",
       "      <td>CAT_14600000</td>\n",
       "      <td>Trash Handling</td>\n",
       "      <td>CLS_14620000</td>\n",
       "      <td>Trash Receptacles</td>\n",
       "      <td>SUB_14620007</td>\n",
       "      <td>Trash Can Lids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29751</th>\n",
       "      <td>234370</td>\n",
       "      <td>HPE Care Pack Proactive Care Service with Defe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Protects your investment and adds valu...</td>\n",
       "      <td>HPE Care Pack Support Plus Service provides co...</td>\n",
       "      <td>HP INC.</td>\n",
       "      <td>U6DV8E</td>\n",
       "      <td>EA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEG_33000000</td>\n",
       "      <td>Services</td>\n",
       "      <td>CAT_33500000</td>\n",
       "      <td>Services - Tech</td>\n",
       "      <td>CLS_33530000</td>\n",
       "      <td>Services - IT</td>\n",
       "      <td>SUB_33530006</td>\n",
       "      <td>Managed Service Plans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29752</th>\n",
       "      <td>63119</td>\n",
       "      <td>VELCRO® Brand Tape Roll, Hook, 2\" x 75', Black</td>\n",
       "      <td>Securely hang posters, decorations, tools and ...</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Safe for use on various surfaces.&lt;/li&gt;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B O X MANAGEMENT, INC.</td>\n",
       "      <td>VEL137</td>\n",
       "      <td>CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEG_900000</td>\n",
       "      <td>Writing, Art &amp; Drafting</td>\n",
       "      <td>CAT_950000</td>\n",
       "      <td>Pens</td>\n",
       "      <td>CLS_951000</td>\n",
       "      <td>Pens - Ink</td>\n",
       "      <td>SUB_951001</td>\n",
       "      <td>Pens - Ballpoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29753</th>\n",
       "      <td>27139</td>\n",
       "      <td>Blue Sky™ Academic Weekly/Monthly Planner, Let...</td>\n",
       "      <td>Keep your events organized with the Blue Sky A...</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;2-page weekly and monthly spreads prov...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BLUE SKY THE COLOR OF IMAGINATION LLC</td>\n",
       "      <td>100135-A23</td>\n",
       "      <td>EA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEG_6000000</td>\n",
       "      <td>Mailing &amp; Shipping</td>\n",
       "      <td>CAT_6600000</td>\n",
       "      <td>Shipping Supplies</td>\n",
       "      <td>CLS_6610000</td>\n",
       "      <td>Protective Supplies</td>\n",
       "      <td>SUB_6610005</td>\n",
       "      <td>Poly Sheeting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29754</th>\n",
       "      <td>126875</td>\n",
       "      <td>Cambro Deli Crocks, 2.7 Qt, Black, Pack Of 6 C...</td>\n",
       "      <td>Make setting up your spread of prepped items s...</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Made from high-impact plastic (FDA app...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAMBRO MFG. CO.</td>\n",
       "      <td>CAMCP27110</td>\n",
       "      <td>CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEG_20000000</td>\n",
       "      <td>Cords &amp; Cables</td>\n",
       "      <td>CAT_20300000</td>\n",
       "      <td>Cables - Computer</td>\n",
       "      <td>CLS_20310000</td>\n",
       "      <td>Cables - Computer Network</td>\n",
       "      <td>SUB_20310001</td>\n",
       "      <td>Cables - Ethernet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29755</th>\n",
       "      <td>223768</td>\n",
       "      <td>DAX 8-1/2x11 Document Woodgrain Frame - 15.50\"...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;ul&gt;&lt;li&gt;Ready-to-use certificate lets you reco...</td>\n",
       "      <td>&lt;p&gt;Document frame features a broad, contempora...</td>\n",
       "      <td>INTERCRAFT INDUSTRIES CORP.</td>\n",
       "      <td>NDWT8511BT</td>\n",
       "      <td>EA</td>\n",
       "      <td>15.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SEG_200000</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>CAT_260000</td>\n",
       "      <td>Desks &amp; Tables</td>\n",
       "      <td>CLS_262000</td>\n",
       "      <td>Desks</td>\n",
       "      <td>SUB_262003</td>\n",
       "      <td>Desks - Corner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29756 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SKU                                          ItemTitle  \\\n",
       "0      131645  Gibson Studio California Mauna 12-Piece Melami...   \n",
       "1       42483  Thronmax X1 Webcam - 30 fps - USB 2.0 - 1 Pack...   \n",
       "2      175678  IOGEAR Thunderbolt 3 USB-C 2m 20Gbps Cable - 6...   \n",
       "3      190527  Hoffman Tech 677-12E-HTI Remanufactured Extra-...   \n",
       "4      145583  Suncast Commercial Paper Recycling Lid For 23-...   \n",
       "...       ...                                                ...   \n",
       "29751  234370  HPE Care Pack Proactive Care Service with Defe...   \n",
       "29752   63119     VELCRO® Brand Tape Roll, Hook, 2\" x 75', Black   \n",
       "29753   27139  Blue Sky™ Academic Weekly/Monthly Planner, Let...   \n",
       "29754  126875  Cambro Deli Crocks, 2.7 Qt, Black, Pack Of 6 C...   \n",
       "29755  223768  DAX 8-1/2x11 Document Woodgrain Frame - 15.50\"...   \n",
       "\n",
       "                                         ItemDescription  \\\n",
       "0      Whether you're dining indoors or outdoors, thi...   \n",
       "1                                                    NaN   \n",
       "2                                                    NaN   \n",
       "3      Remanufactured from an original HP recycled ca...   \n",
       "4      Keep recyclables (where facilities exist) cont...   \n",
       "...                                                  ...   \n",
       "29751                                                NaN   \n",
       "29752  Securely hang posters, decorations, tools and ...   \n",
       "29753  Keep your events organized with the Blue Sky A...   \n",
       "29754  Make setting up your spread of prepped items s...   \n",
       "29755                                                NaN   \n",
       "\n",
       "                                         ItemBulletPoint  \\\n",
       "0      <ul><li>Melamine construction for rugged durab...   \n",
       "1      <ul><li>Video conferencing couldn't get any be...   \n",
       "2      <ul><li>Power delivery up to 100W (20V @5A) us...   \n",
       "3      <ul><li>Comparable to the HP 12A (Q2612D) cart...   \n",
       "4      <ul><li>Made of durable resin.</li> <li>Fits m...   \n",
       "...                                                  ...   \n",
       "29751  <ul><li>Protects your investment and adds valu...   \n",
       "29752  <ul><li>Safe for use on various surfaces.</li>...   \n",
       "29753  <ul><li>2-page weekly and monthly spreads prov...   \n",
       "29754  <ul><li>Made from high-impact plastic (FDA app...   \n",
       "29755  <ul><li>Ready-to-use certificate lets you reco...   \n",
       "\n",
       "                                       ItemDescription_2  \\\n",
       "0                                                    NaN   \n",
       "1      <p><b>Suitable for Most Software </b></p> <p>P...   \n",
       "2      <p><b>Thunderbolt™ 3 - Premium Performance for...   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "29751  HPE Care Pack Support Plus Service provides co...   \n",
       "29752                                                NaN   \n",
       "29753                                                NaN   \n",
       "29754                                                NaN   \n",
       "29755  <p>Document frame features a broad, contempora...   \n",
       "\n",
       "                                Manufacturer   MfrPartNum SellUOM  ItemPrice  \\\n",
       "0                       GIBSON OVERSEAS INC.   995105704M      EA      31.41   \n",
       "1                                   THRONMAX           X1      EA        NaN   \n",
       "2                          ATEN TECHNOLOGIES       GT3C02      EA        NaN   \n",
       "3               IMAGE PROJECTIONS WEST, INC.  677-12E-HTI      EA      62.94   \n",
       "4                        SUNCAST CORPORATION  TCNLID03BLD      EA        NaN   \n",
       "...                                      ...          ...     ...        ...   \n",
       "29751                                HP INC.       U6DV8E      EA        NaN   \n",
       "29752                 B O X MANAGEMENT, INC.       VEL137      CA        NaN   \n",
       "29753  BLUE SKY THE COLOR OF IMAGINATION LLC   100135-A23      EA        NaN   \n",
       "29754                        CAMBRO MFG. CO.   CAMCP27110      CA        NaN   \n",
       "29755            INTERCRAFT INDUSTRIES CORP.   NDWT8511BT      EA      15.39   \n",
       "\n",
       "                                             ItemFactTag       Segment  \\\n",
       "0                                                    NaN  SEG_11000000   \n",
       "1                                                    NaN    SEG_400000   \n",
       "2                                                    NaN  SEG_20000000   \n",
       "3      * Quality toner at a savings * Yields up to 4,...  SEG_24000000   \n",
       "4                                                    NaN  SEG_14000000   \n",
       "...                                                  ...           ...   \n",
       "29751                                                NaN  SEG_33000000   \n",
       "29752                                                NaN    SEG_900000   \n",
       "29753                                                NaN   SEG_6000000   \n",
       "29754                                                NaN  SEG_20000000   \n",
       "29755                                                NaN    SEG_200000   \n",
       "\n",
       "                             Segment Name      Category  \\\n",
       "0                         Food & Beverage  CAT_11500000   \n",
       "1                    Computers & Hardware    CAT_420000   \n",
       "2                          Cords & Cables  CAT_20500000   \n",
       "3      Ink, Toner, Ribbons & 3D Filaments  CAT_24400000   \n",
       "4                              Janitorial  CAT_14600000   \n",
       "...                                   ...           ...   \n",
       "29751                            Services  CAT_33500000   \n",
       "29752             Writing, Art & Drafting    CAT_950000   \n",
       "29753                  Mailing & Shipping   CAT_6600000   \n",
       "29754                      Cords & Cables  CAT_20300000   \n",
       "29755                           Furniture    CAT_260000   \n",
       "\n",
       "                 Category Name         Class                   Class Name  \\\n",
       "0      Food & Beverage Serving  CLS_11540000                    Tableware   \n",
       "1         Computer Accessories    CLS_424000  Calibrators, Webcams & Hubs   \n",
       "2                 Power & Data  CLS_20510000       Cables - Data Transfer   \n",
       "3                        Toner  CLS_24410000                Machine Toner   \n",
       "4               Trash Handling  CLS_14620000            Trash Receptacles   \n",
       "...                        ...           ...                          ...   \n",
       "29751          Services - Tech  CLS_33530000                Services - IT   \n",
       "29752                     Pens    CLS_951000                   Pens - Ink   \n",
       "29753        Shipping Supplies   CLS_6610000          Protective Supplies   \n",
       "29754        Cables - Computer  CLS_20310000    Cables - Computer Network   \n",
       "29755           Desks & Tables    CLS_262000                        Desks   \n",
       "\n",
       "          Sub-Class             Sub-Class Name  \n",
       "0      SUB_11540008          Sets - Dinnerware  \n",
       "1        SUB_424003                    Webcams  \n",
       "2      SUB_20510002              Cables - Data  \n",
       "3       SUB_5070132  Cartridges - Toner, Color  \n",
       "4      SUB_14620007             Trash Can Lids  \n",
       "...             ...                        ...  \n",
       "29751  SUB_33530006      Managed Service Plans  \n",
       "29752    SUB_951001           Pens - Ballpoint  \n",
       "29753   SUB_6610005              Poly Sheeting  \n",
       "29754  SUB_20310001          Cables - Ethernet  \n",
       "29755    SUB_262003             Desks - Corner  \n",
       "\n",
       "[29756 rows x 18 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arvind=pd.read_csv(\"/root/Capstone_Dataset_final.csv\")\n",
    "data  = df_arvind[~(df_arvind[\"SKU\"].isin(outlier_sku))]\n",
    "data = data.reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create Tokens for Product description </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(data):\n",
    "    language = \"en\"\n",
    "    new_data = []\n",
    "    regex = re.compile('<.*?>')\n",
    "    count = 1\n",
    "    segments = []\n",
    "    categories = []\n",
    "    classes = []\n",
    "    sub_classes = []\n",
    "    for index, row in data.iterrows():\n",
    "        sku = row[\"SKU\"]\n",
    "        manu = row[\"Manufacturer\"]\n",
    "        desc = row[\"ItemDescription\"]\n",
    "        desc_2 = row[\"ItemDescription_2\"]\n",
    "        title = row[\"ItemTitle\"]\n",
    "        bullet = row[\"ItemBulletPoint\"]\n",
    "        segment = row[\"Segment\"]\n",
    "        category = row[\"Category\"]\n",
    "        class_ = row[\"Class\"]\n",
    "        sub_class = row[\"Sub-Class\"]\n",
    "        \n",
    "        segments.append(segment)\n",
    "        categories.append(category)\n",
    "        classes.append(class_)\n",
    "        sub_classes.append(sub_class)\n",
    "        \n",
    "        words = []\n",
    "        keywords = []\n",
    "        keywords_2 = []\n",
    "        keywords_3 = []\n",
    "        total_text = \"\"\n",
    "        if(type(desc) is str):\n",
    "            total_text += desc        \n",
    "        total_text = total_text.lower()\n",
    "        total_text = re.sub(r\"\\[(.*?)\\]\", \"\", total_text)  # Remove [+XYZ chars] in content\n",
    "        total_text = re.sub(r\"\\s+\", \" \", total_text)  # Remove multiple spaces in content\n",
    "        total_text = re.sub(r\"\\w+…|…\", \"\", total_text)  # Remove ellipsis (and last word)\n",
    "        total_text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", total_text)  # Replace dash between words\n",
    "        total_text = re.sub(\n",
    "            f\"[{re.escape(string.punctuation)}]\", \"\", total_text\n",
    "        )  # Remove punctuation\n",
    "        \n",
    "        tokens = word_tokenize(total_text)\n",
    "        output = str(count) + \"/\" + str(len(data))\n",
    "        count +=1\n",
    "        print(output, end = \"\\r\")\n",
    "        new_data.append(tokens)\n",
    "    return new_data, segments, categories, classes, sub_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29756/29756\r"
     ]
    }
   ],
   "source": [
    "new_data, train_segments, train_categories, train_classes, train_sub_classes = tokenize_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Vectorize </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(new_data)]\n",
    "model = Doc2Vec(documents, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29756/29756\r"
     ]
    }
   ],
   "source": [
    "start_alpha=0.01\n",
    "X = []\n",
    "count = 1\n",
    "for tokens in new_data:\n",
    "    X.append( model.infer_vector(tokens, alpha=start_alpha))\n",
    "    output = str(count) + \"/\" + str(len(new_data))\n",
    "    print(output, end = \"\\r\")\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Find Cluster Sizes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Segments: 39\n",
      "Unique Categories: 220\n",
      "Unique Classes: 611\n",
      "Unique Sub Classes: 1656\n"
     ]
    }
   ],
   "source": [
    "unique_segments = len(list(set(train_segments)))\n",
    "print(\"Unique Segments:\", unique_segments)\n",
    "\n",
    "unique_categories = len(list(set(train_categories)))\n",
    "print(\"Unique Categories:\", unique_categories)\n",
    "\n",
    "unique_classes = len(list(set(train_classes)))\n",
    "print(\"Unique Classes:\", unique_classes)\n",
    "\n",
    "unique_sub_classes = len(list(set(train_sub_classes)))\n",
    "print(\"Unique Sub Classes:\", unique_sub_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Train Models </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette coefficient: 0.27\n",
      "Silhouette coefficient: 0.28\n",
      "Silhouette coefficient: 0.30\n",
      "Silhouette coefficient: 0.32\n"
     ]
    }
   ],
   "source": [
    "segment_model = Birch(branching_factor=50, n_clusters=unique_segments, threshold=.2, compute_labels=True)\n",
    "segment_model.fit(X)\n",
    "segment_labels = segment_model.labels_\n",
    "print(f\"Silhouette coefficient: {metrics.silhouette_score(X, segment_model.labels_):0.2f}\")\n",
    "\n",
    "category_model = Birch(branching_factor=50, n_clusters=unique_categories, threshold=0.2, compute_labels=True)\n",
    "category_model.fit(X)\n",
    "category_labels = category_model.labels_\n",
    "print(f\"Silhouette coefficient: {metrics.silhouette_score(X, category_model.labels_):0.2f}\")\n",
    "\n",
    "class_model = Birch(branching_factor=50, n_clusters=unique_classes, threshold=0.2, compute_labels=True)\n",
    "class_model.fit(X)\n",
    "class_labels = class_model.labels_\n",
    "print(f\"Silhouette coefficient: {metrics.silhouette_score(X, class_model.labels_):0.2f}\")\n",
    "\n",
    "sub_class_model = Birch(branching_factor=50, n_clusters=unique_sub_classes, threshold=0.2, compute_labels=True)\n",
    "sub_class_model.fit(X)\n",
    "sub_class_labels = sub_class_model.labels_\n",
    "print(f\"Silhouette coefficient: {metrics.silhouette_score(X, sub_class_model.labels_):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Find Cluster Counts and Mappings for Labels </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mapping(labels, train_labels):\n",
    "    unique_elements = list(set(train_labels))\n",
    "\n",
    "    cluster_counts = {}\n",
    "    mapping = {}\n",
    "    for x in range (0, len(unique_elements)):\n",
    "        cluster_counts[x] = list()\n",
    "\n",
    "        mapping[x] = list()\n",
    "\n",
    "    for x in range(len(train_labels)):\n",
    "        cluster_counts[labels[x]].append(train_labels[x])\n",
    "\n",
    "    for x in range(0, len(unique_elements)):\n",
    "        c = Counter(cluster_counts[x])\n",
    "        if(len(c) != 0):\n",
    "            most_common = c.most_common()\n",
    "            mapping[x] = most_common     \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level_count(labels):\n",
    "    count = {}\n",
    "    unique_labels = list(set(labels))\n",
    "    for x in unique_labels:\n",
    "        count[x] = 0\n",
    "        \n",
    "    c = Counter(labels)\n",
    "    for x in c.most_common():\n",
    "        count[x[0]] = x[1]\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_range = 2\n",
    "category_range = 2\n",
    "class_range = 2\n",
    "sub_class_range = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_map = create_mapping(segment_labels, train_segments)\n",
    "category_map = create_mapping(category_labels, train_categories)\n",
    "class_map = create_mapping(class_labels, train_classes)\n",
    "sub_class_map = create_mapping(sub_class_labels, train_sub_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_count = get_level_count(train_segments)\n",
    "category_count = get_level_count(train_categories)\n",
    "class_count = get_level_count(train_classes)\n",
    "sub_class_count = get_level_count(train_sub_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_predictions = segment_labels\n",
    "category_predictions = category_labels\n",
    "class_predictions = class_labels\n",
    "sub_class_predictions = sub_class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Calculate Average Precision and Accuracy </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results - Average over whole dataset\n",
      "\n",
      "Segment Accuracy: 0.34\n",
      "Category Accuracy: 0.28\n",
      "Class Accuracy: 0.27\n",
      "Sub Class Accuracy: 0.30\n",
      "Precision: 0.20\n"
     ]
    }
   ],
   "source": [
    "segment_accuracy = 0\n",
    "category_accuracy = 0\n",
    "class_accuracy = 0\n",
    "sub_class_accuracy = 0\n",
    "precision = 0\n",
    "\n",
    "for x in range(len(segment_predictions)):\n",
    "    segment = train_segments[x]\n",
    "    category = train_categories[x]\n",
    "    class_ = train_classes[x]\n",
    "    sub_class = train_sub_classes[x]\n",
    "    current_precision = 0\n",
    "    segments = list(map(operator.itemgetter(0), segment_map[segment_predictions[x]][0:segment_range]))\n",
    "    if(segment in segments):\n",
    "        segment_accuracy +=1\n",
    "        current_precision += .25\n",
    "\n",
    "    categories = list(map(operator.itemgetter(0), category_map[category_predictions[x]][0:category_range]))\n",
    "    if(category in categories):\n",
    "        category_accuracy +=1\n",
    "        if(current_precision == .25):\n",
    "            current_precision += .25\n",
    "        \n",
    "    classes = list(map(operator.itemgetter(0), class_map[class_predictions[x]][0:class_range]))\n",
    "    if(class_ in classes):\n",
    "        class_accuracy +=1\n",
    "        if(current_precision == .5):\n",
    "            current_precision += .25\n",
    "    \n",
    "    sub_classes = list(map(operator.itemgetter(0), sub_class_map[sub_class_predictions[x]][0:sub_class_range]))\n",
    "    if(sub_class in sub_classes):\n",
    "        sub_class_accuracy +=1\n",
    "        if(current_precision == .75):\n",
    "            current_precision += .25\n",
    "    precision += current_precision\n",
    "    \n",
    "\n",
    "segment_accuracy = segment_accuracy/len(segment_predictions)\n",
    "category_accuracy = category_accuracy/len(category_predictions)\n",
    "class_accuracy = class_accuracy/len(class_predictions)\n",
    "sub_class_accuracy = sub_class_accuracy/len(sub_class_predictions)\n",
    "precision = precision/len(segment_predictions)\n",
    "\n",
    "\n",
    "print(\"Results - Average over whole dataset\")\n",
    "print()\n",
    "print(f\"Segment Accuracy: {segment_accuracy:0.2f}\")\n",
    "print(f\"Category Accuracy: {category_accuracy:0.2f}\")\n",
    "print(f\"Class Accuracy: {class_accuracy:0.2f}\")\n",
    "print(f\"Sub Class Accuracy: {sub_class_accuracy:0.2f}\")\n",
    "print(f\"Precision: {precision:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Calculate Confidence Scores and Output to CSV File </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_score(mapping, prediction, x_val, counts):\n",
    "    confidence = 0\n",
    "    \n",
    "    if(not pd.isna(x_val)):\n",
    "        total = counts[x_val]\n",
    "        index = list(map(operator.itemgetter(0), mapping[prediction])).index(x_val)\n",
    "        cluster_count = mapping[prediction][index][1]\n",
    "        confidence = cluster_count/total\n",
    "\n",
    "        \n",
    "    \n",
    "    return confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_results(data, header, segment_predictions, category_predictions, class_predictions, \n",
    "                   sub_class_predictions, segment_map, category_map, class_map, sub_class_map,\n",
    "                  segment_count, category_count, class_count, sub_class_count):   \n",
    "    results = []\n",
    "    count = 1\n",
    "    for index, row in data.iterrows():\n",
    "        sku = row[\"SKU\"]\n",
    "        title = row[\"ItemTitle\"]\n",
    "        segment = row[\"Segment\"]\n",
    "        category = row[\"Category\"]\n",
    "        class_ = row[\"Class\"]\n",
    "        sub_class = row[\"Sub-Class\"]\n",
    "        segment_name = row[\"Segment Name\"]\n",
    "        category_name = row[\"Category Name\"]\n",
    "        class_name = row[\"Class Name\"]\n",
    "        sub_class_name = row[\"Sub-Class Name\"]\n",
    "\n",
    "\n",
    "        segment_prediction = segment_predictions[index]\n",
    "        segment_confidence = get_confidence_score(segment_map, segment_prediction, segment, segment_count)\n",
    "\n",
    "        category_prediction = category_predictions[index]\n",
    "        category_confidence = get_confidence_score(category_map, category_prediction, category, category_count)\n",
    "\n",
    "        class_prediction = class_predictions[index]\n",
    "        class_confidence = get_confidence_score(class_map, class_prediction, class_, class_count)\n",
    "        \n",
    "\n",
    "        sub_class_prediction = sub_class_predictions[index]\n",
    "        sub_class_confidence = get_confidence_score(sub_class_map, sub_class_prediction, sub_class, sub_class_count)\n",
    "\n",
    "\n",
    "        result = [sku, title, sub_class_name, class_name, category_name, segment_name, \n",
    "                  sub_class_confidence, class_confidence, category_confidence, segment_confidence]\n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "        output = str(count) + \"/\" + str(len(data))\n",
    "        count +=1\n",
    "        print(output, end = \"\\r\")\n",
    "    for i in outlier_accuracy.items():\n",
    "        # Appending result from outlier accuracies that were calculated above\n",
    "        results.append([i[0],list(df_arvind.loc[df_arvind[\"SKU\"]==i[0]][\"ItemTitle\"])[0],i[1][0][0],i[1][1][0],i[1][2][0],i[1][3][0],i[1][0][1],i[1][1][1],i[1][2][1],i[1][3][1]])\n",
    "    with open('results.csv', 'w+', encoding='UTF8', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        # write the header\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # write multiple rows\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(\"Finished: Wrote to CSV file results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Wrote to CSV file results.csv\n"
     ]
    }
   ],
   "source": [
    "header = ['SKU', 'ItemTitle', 'Sub-Class Name', 'Class Name', 'Category Name', 'Segment Name', \n",
    "          'Sub-Class Score', 'Class Score', 'Category Score', 'Segment Score']\n",
    "output_results(data, header, segment_predictions, category_predictions, class_predictions, \n",
    "               sub_class_predictions, segment_map, category_map, \n",
    "               class_map, sub_class_map, segment_count, category_count, class_count,\n",
    "              sub_class_count)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.r5.24xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
